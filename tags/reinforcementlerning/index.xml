<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ReinforcementLerning on MotoShin Blog</title>
    <link>https://motoshin.github.io/tags/reinforcementlerning/</link>
    <description>Recent content in ReinforcementLerning on MotoShin Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sun, 04 Jul 2021 15:50:29 +0900</lastBuildDate><atom:link href="https://motoshin.github.io/tags/reinforcementlerning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Spinning Up の和訳を書いていく</title>
      <link>https://motoshin.github.io/posts/2021-07-04-blog/</link>
      <pubDate>Sun, 04 Jul 2021 15:50:29 +0900</pubDate>
      
      <guid>https://motoshin.github.io/posts/2021-07-04-blog/</guid>
      <description>最近 Spinning Up というものの輪読会的なものを始めることになりました。 そこでその準備のため Spinning Up の和訳をつらつらと書いていこうと思います。 なお、全文ではなく僕が重要そうだな〜と思った部分を和訳してメモしていく感じです。
Part1: Key Concepts in RL 該当ページは ここ です。 主に Key Concepts and Terminology を和訳していきます。
Key Concepts and Terminology Action Space 離散的行動空間 (descrete action space) と連続的行動空間 (continuous action space) があるよ〜的な説明
Policies 方策 (policy) はエージェント (agent) が行動を決定する際に使用されるルールである。 方策は決定論的 (deterministic) な場合は $\mu$ を用いて下記のように記述する。 $$ a_t = \mu(s_t) $$
確率論的 (stochastic) の場合は通常 $\pi$ を用いて下記ように記述する。 $$ a_t \sim \pi(\cdot|s_t) $$
パラメータがある場合は $\theta$ や $\phi$ を用いて下記のように記述する。 $$ a_t = \mu_{\theta}(s_t) $$ $$ a_t \sim \pi_{\theta}(\cdot|s_t) $$</description>
    </item>
    
    <item>
      <title>【メモ】DDPG論文の実験詳細の和訳</title>
      <link>https://motoshin.github.io/posts/2021-02-07/</link>
      <pubDate>Sun, 07 Feb 2021 14:07:58 +0900</pubDate>
      
      <guid>https://motoshin.github.io/posts/2021-02-07/</guid>
      <description>皆さん、明けましておめでとうございます。 本年もどうぞよろしくお願いします。（年明けて１ヶ月経過…）
そんなことは置いといて、今回はブログでもなんでもなく生存報告を兼ねたただのメモです。 DDPGという強化学習アルゴリズムの論文の実験詳細の部分の和訳もメモしたかっただけです。 なので気にしなくて結構です。
論文の和訳 ニューラルネットワークパラメータの学習にはAdam (Kingma &amp;amp; Ba, 2014)を用い、actorとcriticに対してそれぞれ$10^{-4}$と$10^{-3}$の学習率で学習を行った。 Qについては、$10^{-2}$のL2重み減衰を含み、$\gamma = 0.99$の割引係数を使用した。 ソフトターゲットの更新には$\tau = 0.001$を使用した。 ニューラルネットワークは、すべての隠れ層に整流非線形性(Glorot et al., 2011)を使用した。 actorの最終出力層は、アクションを束縛するためにタン層とした。 低次元ネットワークは2つの隠れ層を持ち、それぞれ400と300のユニット(130,000のパラメータ)を持っていた。 ピクセルから学習する際には、各層に32個のフィルターを持つ3つの畳み込み層（プーリングなし）を使用しました。 これに続いて、200ユニット（430,000パラメータ）の完全に接続された2つの層を使用しました。 最終的なレイヤーの重みとバイアスは，低次元の場合は$[-3 \times 10^{-3}, 3 \times 10^{-3}]$，ピクセルの場合は$[3 \times 10^{-4}, 3 \times 10^{-4}]$の一様分布から初期化した。 これは、政策と値の推定値の初期出力がゼロに近いことを保証するためであった。 他のレイヤーは、一様分布$[-\frac{1}{\sqrt{f}}, \frac{1}{\sqrt{f}}]$から初期化されました。 アクションは、完全に接続された層までは含まれなかった。 低次元問題では64、ピクセルでは16のミニバッチサイズで学習した。 リプレイバッファサイズは$10^{6}$を使用した。
参考  DDPGの論文  </description>
    </item>
    
    <item>
      <title>DQNのお勉強中…</title>
      <link>https://motoshin.github.io/posts/2020-11-10-blog/</link>
      <pubDate>Tue, 10 Nov 2020 20:58:37 +0900</pubDate>
      
      <guid>https://motoshin.github.io/posts/2020-11-10-blog/</guid>
      <description>学生の頃ニューラルネットワーク(NN)を全力で避けていました。 そのため、もちろんNNを使った強化学習手法であるDQNも真面目にやったことはありませんでした。 「これはいかん！」と思い立ち今回勉強しようと思いました。 今回のブログはDQNはなんぞやという記事ではなく、実装してみて思ったことや気付いたことなどをただ書き殴るだけのメモ帳替りのものです。 なのでどんどん追記していくかもしれません。 また今度ちゃんとした解説記事書くから許せ…
リポジトリ DQNのチュートリアル
僕が頑張って実装したものです。 お納めください。 今のところ実装してあるタスクはopen ai gymが提供しているcartpoleという環境についてDQNを実装しました。 PyTorchの強化学習チュートリアルをコピペしたcartpole.pyと、自分なりにチュートリアルの内容をクラスなどに分けてプロジェクト化したcartpoleディレクトリを作りました。
結果 一応結果の画像を貼っておきます。 プログラム実行するととても時間がかかるので…
  rewardのグラフ   episode: 500, simulation: 20での結果です。 この結果出すのに2時間くらいかかりました（環境にもよると思いますが）。 ほんとはsimulationを100とか1000とかやりたいけどとんでもなく時間がかかりそう…
気付き 以下、気付きや思ったことをつらつらと書いていきます。
 NNって出力のargmax取るやつだったな〜  なぜかラベルの数値データ１つが出力するものだと認識してた。 強化学習ではどの行動を取るのかなので行動数分の出力になる   Batch Normalization  NNに画像を入力するときは必須らしい フィルターかけまくって偏ったデータを正規化するものって理解しているけどあっているかは微妙   あんまり成績が伸びない…  rewardは22くらいで伸びが止まっている 軽く調べるとみんな200くらいいっている 画像入力とベクトル入力の差かもしれないな、と考察してる   中間層の数とか入力数と出力数とかどうやって決めてるんや…  ここは職人芸ってよく聞くし数を重ねるしかないのかな〜   画像を入力にしているためウィンドウをレンダリングする必要がる  画面がないAWSとかの環境で回しっぱができない やはりベクトル入力がベストかな〜 なぜPyTorchのチュートリアルは画像入力なんだ…    参考  PyTorch 【GIF】初心者のためのCNNからバッチノーマライゼーションとその仲間たちまでの解説  </description>
    </item>
    
  </channel>
</rss>
