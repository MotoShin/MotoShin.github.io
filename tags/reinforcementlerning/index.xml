<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ReinforcementLerning on MotoShin Blog</title>
    <link>https://motoshin.github.io/tags/reinforcementlerning/</link>
    <description>Recent content in ReinforcementLerning on MotoShin Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sun, 07 Feb 2021 14:07:58 +0900</lastBuildDate>
    
	<atom:link href="https://motoshin.github.io/tags/reinforcementlerning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>【メモ】DDPG論文の実験詳細の和訳</title>
      <link>https://motoshin.github.io/posts/2021-02-07/</link>
      <pubDate>Sun, 07 Feb 2021 14:07:58 +0900</pubDate>
      
      <guid>https://motoshin.github.io/posts/2021-02-07/</guid>
      <description>皆さん、明けましておめでとうございます。 本年もどうぞよろしくお願いします。（年明けて１ヶ月経過…）
そんなことは置いといて、今回はブログでもなんでもなく生存報告を兼ねたただのメモです。 DDPGという強化学習アルゴリズムの論文の実験詳細の部分の和訳もメモしたかっただけです。 なので気にしなくて結構です。
論文の和訳 ニューラルネットワークパラメータの学習にはAdam (Kingma &amp;amp; Ba, 2014)を用い、actorとcriticに対してそれぞれ$10^{-4}$と$10^{-3}$の学習率で学習を行った。 Qについては、$10^{-2}$のL2重み減衰を含み、$\gamma = 0.99$の割引係数を使用した。 ソフトターゲットの更新には$\tau = 0.001$を使用した。 ニューラルネットワークは、すべての隠れ層に整流非線形性(Glorot et al., 2011)を使用した。 actorの最終出力層は、アクションを束縛するためにタン層とした。 低次元ネットワークは2つの隠れ層を持ち、それぞれ400と300のユニット(130,000のパラメータ)を持っていた。 ピクセルから学習する際には、各層に32個のフィルターを持つ3つの畳み込み層（プーリングなし）を使用しました。 これに続いて、200ユニット（430,000パラメータ）の完全に接続された2つの層を使用しました。 最終的なレイヤーの重みとバイアスは，低次元の場合は$[-3 \times 10^{-3}, 3 \times 10^{-3}]$，ピクセルの場合は$[3 \times 10^{-4}, 3 \times 10^{-4}]$の一様分布から初期化した。 これは、政策と値の推定値の初期出力がゼロに近いことを保証するためであった。 他のレイヤーは、一様分布$[-\frac{1}{\sqrt{f}}, \frac{1}{\sqrt{f}}]$から初期化されました。 アクションは、完全に接続された層までは含まれなかった。 低次元問題では64、ピクセルでは16のミニバッチサイズで学習した。 リプレイバッファサイズは$10^{6}$を使用した。
参考  DDPGの論文  </description>
    </item>
    
  </channel>
</rss>