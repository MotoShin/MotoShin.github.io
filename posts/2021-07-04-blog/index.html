<!doctype html>
<html lang="en-us">
  <head>
    <title>Spinning Up の和訳を書いていく // MotoShin Blog</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.76.4" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="MotoShin" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="https://motoshin.github.io/css/main.min.4a7ec8660f9a44b08c4da97c5f2e31b1192df1d4d0322e65c0dbbc6ecb1b863f.css" />

    
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Spinning Up の和訳を書いていく"/>
<meta name="twitter:description" content="最近 Spinning Up というものの輪読会的なものを始めることになりました。 そこでその準備のため Spinning Up の和訳をつらつらと書いていこうと思います。 なお、全文ではなく僕が重要そうだな〜と思った部分を和訳してメモしていく感じです。
Part1: Key Concepts in RL 該当ページは ここ です。 主に Key Concepts and Terminology を和訳していきます。
Key Concepts and Terminology Action Space 離散的行動空間 (descrete action space) と連続的行動空間 (continuous action space) があるよ〜的な説明
Policies 方策 (policy) はエージェント (agent) が行動を決定する際に使用されるルールである。 方策は決定論的 (deterministic) な場合は $\mu$ を用いて下記のように記述する。 $$ a_t = \mu(s_t) $$
確率論的 (stochastic) の場合は通常 $\pi$ を用いて下記ように記述する。 $$ a_t \sim \pi(\cdot|s_t) $$
パラメータがある場合は $\theta$ や $\phi$ を用いて下記のように記述する。 $$ a_t = \mu_{\theta}(s_t) $$ $$ a_t \sim \pi_{\theta}(\cdot|s_t) $$"/>

    <meta property="og:title" content="Spinning Up の和訳を書いていく" />
<meta property="og:description" content="最近 Spinning Up というものの輪読会的なものを始めることになりました。 そこでその準備のため Spinning Up の和訳をつらつらと書いていこうと思います。 なお、全文ではなく僕が重要そうだな〜と思った部分を和訳してメモしていく感じです。
Part1: Key Concepts in RL 該当ページは ここ です。 主に Key Concepts and Terminology を和訳していきます。
Key Concepts and Terminology Action Space 離散的行動空間 (descrete action space) と連続的行動空間 (continuous action space) があるよ〜的な説明
Policies 方策 (policy) はエージェント (agent) が行動を決定する際に使用されるルールである。 方策は決定論的 (deterministic) な場合は $\mu$ を用いて下記のように記述する。 $$ a_t = \mu(s_t) $$
確率論的 (stochastic) の場合は通常 $\pi$ を用いて下記ように記述する。 $$ a_t \sim \pi(\cdot|s_t) $$
パラメータがある場合は $\theta$ や $\phi$ を用いて下記のように記述する。 $$ a_t = \mu_{\theta}(s_t) $$ $$ a_t \sim \pi_{\theta}(\cdot|s_t) $$" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://motoshin.github.io/posts/2021-07-04-blog/" />
<meta property="article:published_time" content="2021-07-04T15:50:29+09:00" />
<meta property="article:modified_time" content="2021-07-04T15:50:29+09:00" />


  </head>
  <body>
    <header class="app-header">
      <a href="https://motoshin.github.io"><img class="app-header-avatar" src="/avatar.jpg" alt="MotoShin" /></a>
      <h1>MotoShin Blog</h1>
      <p>日々の事をつらつらと書いてゆきます。</p>
      <div class="app-header-social">
        
          <a target="_blank" href="https://github.com/MotoShin" rel="noreferrer noopener"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github">
  <title>github</title>
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
</svg></a>
        
      </div>
    </header>
    <main class="app-container">
      
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {delimiters: [
        {left: "$$", right: "$$", display: true},
        {left: "$", right: "$", display: false}]
        });
    });
</script>

      
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">Spinning Up の和訳を書いていく</h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Jul 4, 2021
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          2 min read
        </div><div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag">
  <title>tag</title>
  <path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7.01" y2="7"></line>
</svg>
          <a class="tag" href="https://motoshin.github.io/tags/spinningup/">SpinningUp</a><a class="tag" href="https://motoshin.github.io/tags/reinforcementlerning/">ReinforcementLerning</a></div></div>
    </header>
    <div class="post-content">
      <p>最近 <a href="https://spinningup.openai.com/en/latest/index.html">Spinning Up</a> というものの輪読会的なものを始めることになりました。
そこでその準備のため Spinning Up の和訳をつらつらと書いていこうと思います。
なお、全文ではなく僕が重要そうだな〜と思った部分を和訳してメモしていく感じです。</p>
<h1 id="part1-key-concepts-in-rl">Part1: Key Concepts in RL</h1>
<p>該当ページは <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#key-concepts-and-terminology">ここ</a> です。
主に <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#key-concepts-and-terminology">Key Concepts and Terminology</a> を和訳していきます。</p>
<h2 id="key-concepts-and-terminology">Key Concepts and Terminology</h2>
<h3 id="action-space">Action Space</h3>
<p>離散的行動空間 (descrete action space) と連続的行動空間 (continuous action space) があるよ〜的な説明</p>
<h3 id="policies">Policies</h3>
<p>方策 (policy) はエージェント (agent) が行動を決定する際に使用されるルールである。
方策は決定論的 (deterministic) な場合は $\mu$ を用いて下記のように記述する。
$$
a_t = \mu(s_t)
$$</p>
<p>確率論的 (stochastic) の場合は通常 $\pi$ を用いて下記ように記述する。
$$
a_t \sim \pi(\cdot|s_t)
$$</p>
<p>パラメータがある場合は $\theta$ や $\phi$ を用いて下記のように記述する。
$$
a_t = \mu_{\theta}(s_t)
$$
$$
a_t \sim \pi_{\theta}(\cdot|s_t)
$$</p>
<h4 id="deterministic-policies">Deterministic Policies</h4>
<p>pytorchでの実装例が書いてあるだけ。</p>
<h4 id="stochastic-policies">Stochastic Policies</h4>
<p>深層強化学習で最もよく使われるのは categorical policies と diagonal gausian policies である。
categorical policies は離散的行動空間の場合に使用され、diagonal gausian policies は連続的行動空間の場合に使用される。</p>
<p>確率論的な方策の使用やトレーニングにおいて重要なことは</p>
<ul>
<li>方策によって選択する行動をサンプリングすること</li>
<li>特定の行動 $a$ の対数尤度を計算すること $log\pi_{\theta}(a|s)$</li>
</ul>
<p>である。</p>
<h5 id="categorical-policies">Categorical Policies</h5>
<p>クラス分類のニューラルネットワークと似ている。
最後の層に softmax 層を作って出力を確率に変換する。</p>
<p>対数尤度の求め方は下記の通り<br>
最後の層の出力を $P_{\theta}(s)$ とすると
$$
log\pi_{\theta}(a|s) = log\bigl[P_{\theta}(s)\bigr]_a
$$
となる。</p>
<h5 id="categorical-policies-1">Categorical Policies</h5>
<p>あとで追記する。</p>
<h3 id="trajectories">Trajectories</h3>
<p>状態・行動シーケンスを $\tau$ を使って下記のように記述する。
$$
\tau = (s_0, a_0, s_1, a_1, \dots)
$$</p>
<p>初期状態 $s_0$ が初期状態分布にしたがってランダムで設定されることを下記のように記述する。
$$
s_0 \sim \rho_0(\cdot)
$$</p>
<p>環境の状態遷移が決定論的な場合は
$$
s_{t+1} = f(s_t, a_t)
$$
確率論的な場合は
$$
s_{t+1} \sim P(\cdot|s_t, a_t)
$$
と記述する。<br>
行動はエージェントの方策に準拠する。Trajectory は episode や rollouts と呼ばれることもある。</p>
<h3 id="reward-and-return">Reward and Return</h3>
<p>報酬関数 $R$ は強化学習において非常に重要です。それはタスクの現在の状態、選択した行動、そしてタスクの次の状態に依存している。
$$
r_t = R(s_t, a_t, s_{t+1})
$$
多くの場合は現在の状態 $r_t = R(s_t)$ や現在の状態と行動のペア $r_t = R(s_t, a_t)$ への依存に単純化されている。</p>
<p>エージェントの目標は軌跡の累計報酬を最大化することであるが、これは実際にはいくつかの意味を持つ。
これらのいずれのケースにおいてもすべて $R(\tau)$ で表記しますが、どのケースを意味しているかは文脈から明らかになるか、そもそも問題になることはないだろう（同じ式がすべてのケースに適用されるからである）。</p>
<p>有限時区間 (finit-horizon) の報酬和は、単純な固定幅のエピソードで得られる報酬の合計である。
$$
R(\tau) = \sum_{T}^{t=0} r_t
$$</p>
<p>その他の返値としては、無限時区間割引報酬和 (unfinit-horizon return) である。これはエージェントがこれまでに得た報酬の合計ですが獲得するタイミングが未来であれば未来であるほど割引率として小さな値がかけられる。
割引率 $\gamma$ は $\gamma \in (0,1)$ の範囲である。
$$
R(\tau) = \sum_{\infty}^{t=0} \gamma^{t}r_t
$$</p>
<p>割引率が必要な理由としては1エピソードが有限ステップで終わらないようなタスクでは、報酬和が有限に収束しない可能性があるためである。
割引率があると報酬和は有限和に収束する。</p>
<p>割引前の報酬和を最適化するためにアルゴリズムを設定することはよくあるが、価値関数を推定する際には割引係数を使用する。</p>
<h3 id="the-rl-problem">The RL Problem</h3>
<p>どのような報酬和の計算方法を選択したとしても (finit-horizon retrun や unfinit-horizon retrun) 、
そしてどんな方策を選択したとしても、強化学習の目的はエージェントが報酬和の期待値を最大化するような行動を選択する方策を選択することである。</p>
<p>報酬和の期待値の話をするには、まず軌跡の確率分布の話をしなければいけない。</p>
<p>環境の遷移と方策の両方共に確率的だったと仮定する。
この場合、$T$-stepの軌跡の確率は下記のようになる。
$$
P(\tau|\pi) = \rho_0(s_0)\prod_{T-1}^{t=0} P(s_{t+1}|s_t,a_t)\pi(a_t|s_t)
$$</p>
<p>報酬和の期待値は $J(\pi)$ と表記され下記のように定義される。
$$
J(\pi) = \int_{\tau} P(\tau|\pi)R(\tau) = \underset{\tau\sim\pi}{\Epsilon} \bigl[R(\tau)\bigr]
$$</p>
<p>そして強化学習の最適化問題は、次のように表すことができます。
$$
\pi^{*} = \argmax_{\pi} J(\pi)
$$
このとき、$\pi^{*}$ は最適方策となる。</p>
<p><em>続きはまた後日に</em></p>
<h1 id="参考">参考</h1>
<ul>
<li>慶應技術大学理工学部 櫻井彰人. &ldquo;<a href="http://www.sakurai.comp.ae.keio.ac.jp/classes/infosem-class/2009/12RL.pdf">情報意味論(12) 強化学習</a>&rdquo;, (参照2021-07-08)</li>
</ul>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
