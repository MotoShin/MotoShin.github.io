<!doctype html>
<html lang="en-us">
  <head>
    <title>Spinning Up の和訳を書いていく // MotoShin Blog</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.68.3" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="MotoShin" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="https://motoshin.github.io/css/main.min.4a7ec8660f9a44b08c4da97c5f2e31b1192df1d4d0322e65c0dbbc6ecb1b863f.css" />

    
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Spinning Up の和訳を書いていく"/>
<meta name="twitter:description" content="最近 Spinning Up というものの輪読会的なものを始めることになりました。 そこでその準備のため Spinning Up の和訳をつらつらと書いていこうと思います。 なお、全文ではなく僕が重要そうだな〜と思った部分を和訳してメモしていく感じです。
Part1: Key Concepts in RL 該当ページは ここ です。 主に Key Concepts and Terminology を和訳していきます。
Key Concepts and Terminology Action Space 離散的行動空間 (descrete action space) と連続的行動空間 (continuous action space) があるよ〜的な説明
Policies 方策 (policy) はエージェント (agent) が行動を決定する際に使用されるルールである。 方策は決定論的 (deterministic) な場合は $\mu$ を用いて下記のように記述する。 $$ a_t = \mu(s_t) $$
確率論的 (stochastic) の場合は通常 $\pi$ を用いて下記ように記述する。 $$ a_t \sim \pi(\cdot|s_t) $$
パラメータがある場合は $\theta$ や $\phi$ を用いて下記のように記述する。 $$ a_t = \mu_{\theta}(s_t) $$ $$ a_t \sim \pi_{\theta}(\cdot|s_t) $$"/>

    <meta property="og:title" content="Spinning Up の和訳を書いていく" />
<meta property="og:description" content="最近 Spinning Up というものの輪読会的なものを始めることになりました。 そこでその準備のため Spinning Up の和訳をつらつらと書いていこうと思います。 なお、全文ではなく僕が重要そうだな〜と思った部分を和訳してメモしていく感じです。
Part1: Key Concepts in RL 該当ページは ここ です。 主に Key Concepts and Terminology を和訳していきます。
Key Concepts and Terminology Action Space 離散的行動空間 (descrete action space) と連続的行動空間 (continuous action space) があるよ〜的な説明
Policies 方策 (policy) はエージェント (agent) が行動を決定する際に使用されるルールである。 方策は決定論的 (deterministic) な場合は $\mu$ を用いて下記のように記述する。 $$ a_t = \mu(s_t) $$
確率論的 (stochastic) の場合は通常 $\pi$ を用いて下記ように記述する。 $$ a_t \sim \pi(\cdot|s_t) $$
パラメータがある場合は $\theta$ や $\phi$ を用いて下記のように記述する。 $$ a_t = \mu_{\theta}(s_t) $$ $$ a_t \sim \pi_{\theta}(\cdot|s_t) $$" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://motoshin.github.io/posts/2021-07-04-blog/" />
<meta property="article:published_time" content="2021-07-04T15:50:29+09:00" />
<meta property="article:modified_time" content="2021-07-04T15:50:29+09:00" />


  </head>
  <body>
    <header class="app-header">
      <a href="https://motoshin.github.io"><img class="app-header-avatar" src="/avatar.jpg" alt="MotoShin" /></a>
      <h1>MotoShin Blog</h1>
      <p>日々の事をつらつらと書いてゆきます。</p>
      <div class="app-header-social">
        
          <a target="_blank" href="https://github.com/MotoShin" rel="noreferrer noopener"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github">
  <title>github</title>
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
</svg></a>
        
      </div>
    </header>
    <main class="app-container">
      
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {delimiters: [
        {left: "$$", right: "$$", display: true},
        {left: "$", right: "$", display: false}]
        });
    });
</script>

      
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">Spinning Up の和訳を書いていく</h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Jul 4, 2021
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          3 min read
        </div><div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag">
  <title>tag</title>
  <path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7.01" y2="7"></line>
</svg>
          <a class="tag" href="https://motoshin.github.io/tags/spinningup/">SpinningUp</a><a class="tag" href="https://motoshin.github.io/tags/reinforcementlerning/">ReinforcementLerning</a></div></div>
    </header>
    <div class="post-content">
      <p>最近 <a href="https://spinningup.openai.com/en/latest/index.html">Spinning Up</a> というものの輪読会的なものを始めることになりました。
そこでその準備のため Spinning Up の和訳をつらつらと書いていこうと思います。
なお、全文ではなく僕が重要そうだな〜と思った部分を和訳してメモしていく感じです。</p>
<h1 id="part1-key-concepts-in-rl">Part1: Key Concepts in RL</h1>
<p>該当ページは <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#key-concepts-and-terminology">ここ</a> です。
主に <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#key-concepts-and-terminology">Key Concepts and Terminology</a> を和訳していきます。</p>
<h2 id="key-concepts-and-terminology">Key Concepts and Terminology</h2>
<h3 id="action-space">Action Space</h3>
<p>離散的行動空間 (descrete action space) と連続的行動空間 (continuous action space) があるよ〜的な説明</p>
<h3 id="policies">Policies</h3>
<p>方策 (policy) はエージェント (agent) が行動を決定する際に使用されるルールである。
方策は決定論的 (deterministic) な場合は $\mu$ を用いて下記のように記述する。
$$
a_t = \mu(s_t)
$$</p>
<p>確率論的 (stochastic) の場合は通常 $\pi$ を用いて下記ように記述する。
$$
a_t \sim \pi(\cdot|s_t)
$$</p>
<p>パラメータがある場合は $\theta$ や $\phi$ を用いて下記のように記述する。
$$
a_t = \mu_{\theta}(s_t)
$$
$$
a_t \sim \pi_{\theta}(\cdot|s_t)
$$</p>
<h4 id="deterministic-policies">Deterministic Policies</h4>
<p>pytorchでの実装例が書いてあるだけ。</p>
<h4 id="stochastic-policies">Stochastic Policies</h4>
<p>深層強化学習で最もよく使われるのは categorical policies と diagonal gausian policies である。
categorical policies は離散的行動空間の場合に使用され、diagonal gausian policies は連続的行動空間の場合に使用される。</p>
<p>確率論的な方策の使用やトレーニングにおいて重要なことは</p>
<ul>
<li>方策によって選択する行動をサンプリングすること</li>
<li>特定の行動 $a$ の対数尤度を計算すること $log\pi_{\theta}(a|s)$</li>
</ul>
<p>である。</p>
<h5 id="categorical-policies">Categorical Policies</h5>
<p>クラス分類のニューラルネットワークと似ている。
最後の層に softmax 層を作って出力を確率に変換する。</p>
<p>対数尤度の求め方は下記の通り<br>
最後の層の出力を $P_{\theta}(s)$ とすると
$$
log\pi_{\theta}(a|s) = log\bigl[P_{\theta}(s)\bigr]_a
$$
となる。</p>
<h5 id="categorical-policies-1">Categorical Policies</h5>
<p>あとで追記する。</p>
<h3 id="trajectories">Trajectories</h3>
<p>状態・行動シーケンスを $\tau$ を使って下記のように記述する。
$$
\tau = (s_0, a_0, s_1, a_1, \dots)
$$</p>
<p>初期状態 $s_0$ が初期状態分布にしたがってランダムで設定されることを下記のように記述する。
$$
s_0 \sim \rho_0(\cdot)
$$</p>
<p>環境の状態遷移が決定論的な場合は
$$
s_{t+1} = f(s_t, a_t)
$$
確率論的な場合は
$$
s_{t+1} \sim P(\cdot|s_t, a_t)
$$
と記述する。<br>
行動はエージェントの方策に準拠する。Trajectory は episode や rollouts と呼ばれることもある。</p>
<h3 id="reward-and-return">Reward and Return</h3>
<p>報酬関数 $R$ は強化学習において非常に重要です。それはタスクの現在の状態、選択した行動、そしてタスクの次の状態に依存している。
$$
r_t = R(s_t, a_t, s_{t+1})
$$
多くの場合は現在の状態 $r_t = R(s_t)$ や現在の状態と行動のペア $r_t = R(s_t, a_t)$ への依存に単純化されている。</p>
<p>エージェントの目標は軌跡の累計報酬を最大化することであるが、これは実際にはいくつかの意味を持つ。
これらのいずれのケースにおいてもすべて $R(\tau)$ で表記しますが、どのケースを意味しているかは文脈から明らかになるか、そもそも問題になることはないだろう（同じ式がすべてのケースに適用されるからである）。</p>
<p>有限時区間 (finit-horizon) の報酬和は、単純な固定幅のエピソードで得られる報酬の合計である。
$$
R(\tau) = \sum_{T}^{t=0} r_t
$$</p>
<p>その他の返値としては、無限時区間割引報酬和 (unfinit-horizon return) である。これはエージェントがこれまでに得た報酬の合計ですが獲得するタイミングが未来であれば未来であるほど割引率として小さな値がかけられる。
割引率 $\gamma$ は $\gamma \in (0,1)$ の範囲である。
$$
R(\tau) = \sum_{\infty}^{t=0} \gamma^{t}r_t
$$</p>
<p>割引率が必要な理由としては1エピソードが有限ステップで終わらないようなタスクでは、報酬和が有限に収束しない可能性があるためである。
割引率があると報酬和は有限和に収束する。</p>
<p>割引前の報酬和を最適化するためにアルゴリズムを設定することはよくあるが、価値関数を推定する際には割引係数を使用する。</p>
<h3 id="the-rl-problem">The RL Problem</h3>
<p>どのような報酬和の計算方法を選択したとしても (finit-horizon retrun や unfinit-horizon retrun) 、
そしてどんな方策を選択したとしても、強化学習の目的はエージェントが報酬和の期待値を最大化するような行動を選択する方策を選択することである。</p>
<p>報酬和の期待値の話をするには、まず軌跡の確率分布の話をしなければいけない。</p>
<p>環境の遷移と方策の両方共に確率的だったと仮定する。
この場合、$T$-stepの軌跡の確率は下記のようになる。
$$
P(\tau|\pi) = \rho_0(s_0)\prod_{t=1}^{T-1} P(s_{t+1}|s_t,a_t)\pi(a_t|s_t)
$$</p>
<p>報酬和の期待値は $J(\pi)$ と表記され下記のように定義される。
$$
J(\pi) = \int_{\tau} P(\tau|\pi)R(\tau) = \underset{\tau\sim\pi}{\Epsilon} \bigl[R(\tau)\bigr]
$$</p>
<p>そして強化学習の最適化問題は、次のように表すことができます。
$$
\pi^{*} = \argmax_{\pi} J(\pi)
$$
このとき、$\pi^{*}$ は最適方策となる。</p>
<h3 id="value-function">Value Function</h3>
<p>多くの場合、状態の価値または状態と行動のペアの価値を知っておくと便利である。
価値とは、その状態または状態と行動のペアでタスクを開始して特定の方策に従って行動した場合の報酬和の期待値を意味している。
価値関数はほぼすべての強化学習アルゴリズムの中でなんらの方法で使用される。</p>
<p>ここでは4つの主な価値関数について説明する。</p>
<ol>
<li>on-policy の価値関数 $V^{\pi}(s)$ は、ある状態 $s$ からスタートして特定の方策 $\pi$ に常に従って行動した場合の報酬和の期待値を返す。
$$
V^{\pi}(s) = \underset{\tau\sim\pi}{\Epsilon} \bigl[R(\tau)|s_0=s\bigr]
$$</li>
<li>on-policy の行動価値関数 $Q^{\pi}(s,a)$ は、ある状態 $s$ において任意の行動 $a$ をとって開始してそれ以降は方策 $\pi$ に従って行動を選択し続けた場合の報酬和の期待値を返す。
$$
Q^{\pi}(s,a) = \underset{\tau\sim\pi}{\Epsilon} \bigl[R(\tau)|s_0=s,a_0=a\bigr]
$$</li>
<li>最適な価値関数 $V^*(s)$ は、ある状態 $s$ からスタートして最適な方策に従って行動した場合の報酬和の期待値を返す。
$$
V^*(s) = \max_{\pi} \underset{\tau\sim\pi}{\Epsilon} \bigl[R(\tau)|s_{0}=s\bigr]
$$</li>
<li>最適な行動価値関数 $Q^*(s,a)$ は、ある状態 $s$ において任意の行動 $a$ をとって開始してそれ以降は最適な方策に従って行動を選択し続けた場合の報酬和の期待値を返す。
$$
Q^*(s,a) = \max_{\pi} \underset{\tau\sim\pi}{\Epsilon} \bigl[R(\tau)|s_{0}=s,a_{0}=a\bigr]
$$</li>
</ol>
<h5 id="知っておくといいこと">知っておくといいこと</h5>
<p>価値関数と行動価値関数の間には高頻度で発生する重要な関係がある。
$$
V^\pi(s) = \underset{a\sim\pi}{\Epsilon}\bigl[Q^\pi(s,a)\bigr]
$$
と
$$
V^*(s) = \max_a Q^*(s,a)
$$
である。<br>
これらの関係性はこれまで与えられた定義から求めることができます。
あなたはこれらを証明できますか？</p>
<h3 id="the-optimal-q-function-and-the-optimal-action">The Optimal Q-Function and the Optimal Action</h3>
<p>最適な行動価値関数 $Q^*(s,a)$ と最適な方策によって選択された行動の間には重要な関係がある。
$Q^*(s,a)$ は状態 $s$ で任意の行動 $a$ でタスクを初め、その後に最適方策に従って行動を選択した場合の報酬和の期待値を返す、と定義する。</p>
<p>状態 $s$ での最適な方策は、$s$ でのスタートからの報酬和の期待値を最大化する行動を選択することである。
その結果、$Q^* $ があれば最適な行動 $a^*(s)$ を以下の方法で直接得ることができる。
$$
a^*(s) = \argmax_{a} Q^*(s,a)
$$</p>
<p><strong>Note</strong>: $Q^*(s,a)$ を最大化する行動が複数存在する場合は、それらの行動はすべて最適な行動であり、最適な方策はそれらの行動をランダムに選択することができる。
しかし、ある行動を決定論的に選択する最適な方策は必ず存在する。</p>
<h3 id="bellman-equations">Bellman Equations</h3>
<p>4つの価値関数は、いずれもベルマン方程式と呼ばれる特殊な自己無撞着方程式 (self-consistency) に従う。
ベルマン方程式の基本的な考え方は次の通りです。</p>
<blockquote>
<p>スタート地点の価値とは、そこにいることで得られるであろう報酬と、次に降り立つ場所の価値を足したものである。</p>
</blockquote>
<p>on-policyの価値関数のベルマン方程式は下記のようになる。
$$
V^{\pi}(s) = \underset{s^{\prime}\sim P}{\underset{a\sim\pi}{\Epsilon}}\bigl[r(s,a)+\gamma V^\pi(s^{\prime})\bigr]
$$
$$
Q^{\pi}(s,a) = \underset{s^{\prime}\sim P}{\Epsilon}\Bigl[r(s,a)+\gamma\underset{a^{\prime}\sim\pi}{\Epsilon}\bigl[Q^\pi(s^{\prime},a^{\prime})\bigr]\Bigr]
$$</p>
<p>上記の $s^{\prime}\sim P$ は $s^{\prime}\sim P(\cdot|s,a)$ を省略したものであり、次状態 $s'$ が環境の遷移規則からサンプリングされていることを示している。
上記の $a\sim\pi$ は $a\sim\pi(\cdot|s)$ を省略したものであり、$a^{\prime}\sim\pi$ は $a^{\prime}\sim\pi(\cdot|s^{\prime})$ を省略したものである。</p>
<p>ベルマン方程式の最適価値関数は下記のように記述する。
$$
V^*(s) = \max_a \underset{s^{\prime}\sim P}{\Epsilon}\bigl[r(s,a)+\gamma V^*(s^{\prime})\bigr]
$$
$$
Q^*(s,a) = \underset{s^{\prime}\sim P}{\Epsilon}\Bigl[r(s,a)+\gamma\max_{a^{\prime}} Q^*(s^{\prime},a^{\prime})\Bigr]
$$</p>
<p>on-policyの価値関数と最適な価値関数の重要な違いは、行動選択の際に $\max$ がないかあるかの違いである。
これは、エージェントが行動を選択する際に、最適な行動をとるためには最高の価値を得ることができる行動を選択しなければならないという事実を反映している。</p>
<h5 id="知っておくといいこと-1">知っておくといいこと</h5>
<p>&ldquo;ベルマン・バックアップ (Bellman backup)&ldquo;という言葉は強化学習の文献によく出てくる。
ベルマン・バックアップとは、ベルマン方程式の右辺である「報酬 $+$ 次の値」のことである。</p>
<h3 id="advantage-functions">Advantage Functions</h3>
<p>強化学習では、ある行動が絶対的に優れているかどうかではなく、<strong>平均的</strong>にどれだけ優れているかを説明しる必要がある。
つまり、その行動の相対的な優位性を知りたいのである。
この概念を正確に表現するために、<strong>advantage関数</strong> ( <strong>advantage function</strong> ) がある。</p>
<p>状態 $s$ において特定の行動 $a$ をとるという特定の方策 $\pi$ に対応するadvantage関数 $A^{\pi}(s,a)$ は、
その後ずっとランダムに行動を選択することよりも、$\pi$ の従って行動すると場合の方が、どれだけ効率的かを表している。
$$
A^{\pi}(s,a) = Q^{\pi}(s,a) - V^{\pi}(s)
$$</p>
<h5 id="知っておくといいこと-2">知っておくといいこと</h5>
<p>これについてはあとで詳しく説明するが、advantage関数は方策勾配法によって非常に重要である。</p>
<h3 id="optional-formalism">(Optional) Formalism</h3>
<p>ここまでは、エージェントの環境について非公式に説明してきたが、文献を調べようとするとこの設定のための標準的な数学的定義に遭遇する可能性がある。<br>
<strong>マルコフ決定過程</strong>,<strong>Markov Decision Processes</strong> (MDPs) は5つの tuple で $\langle S,A,R,P,\rho_0 \rangle$ と表され、それぞれは下記のような意味がある。</p>
<ul>
<li>$S$ は有効な状態の集合である。</li>
<li>$A$ は有効な行動の集合である。</li>
<li>$R : S \times A \times S \rarr \reals$ は報酬関数であり次のように表される。$r_t=R(s_t,a_t,s_{t+1})$</li>
<li>$P : S \times A \rarr {\cal P}(S)$ は遷移確率関数であり次のように表せる : $P(s^{\prime}|s,a)$ 、この式が意味していることが状態 $s$ で行動 $a$ をとり次状態 $s^{\prime}$ に遷移する確率を意味している。</li>
<li>そして $\rho_0$ は開始状態の分布となる</li>
</ul>
<h1 id="参考">参考</h1>
<ul>
<li>慶應技術大学理工学部 櫻井彰人. &ldquo;<a href="http://www.sakurai.comp.ae.keio.ac.jp/classes/infosem-class/2009/12RL.pdf">情報意味論(12) 強化学習</a>&quot;, (参照2021-07-08)</li>
</ul>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
